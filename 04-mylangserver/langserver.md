pip install langserve pip install "langserve[all]"

pip install "langserve[all]" langchain-openai python-dotenv

verify: python -c "import langserve; print(langserve.version)"

server.py from fastapi import FastAPI from langserve import add_routes from langchain_openai import ChatOpenAI

app = FastAPI()

model = ChatOpenAI( model="deepseek-chat", api_key="your_key", base_url="https://api.deepseek.com" )

add_routes(app, model, path="/deepseek")

run: uvicorn server:app --reload --port 8000

call local API: POST http://localhost:8000/deepseek/invoke { "input": "è®²ä¸ªç¬‘è¯" }


Below is a complete, production-ready LangServe + DeepSeek API server, with:

âœ… LangServe
âœ… DeepSeek (OpenAI-compatible API)
âœ… Streaming endpoint
âœ… /invoke synchronous endpoint
âœ… /stream streaming endpoint
âœ… .env API key loading
âœ… FastAPI + Uvicorn
âœ… Modern LangChain 0.2+ syntax
âœ… Works on Linux / Mac / Windows

You can run this locally and expose your own DeepSeek-powered API.

ğŸš€ 1. Install dependencies
pip install "langserve[all]" langchain-openai python-dotenv

ğŸ“ 2. Project Structure
deepseek_langserve/
â”‚
â”œâ”€â”€ server.py
â”œâ”€â”€ chain.py
â””â”€â”€ .env

ğŸ§© 3. .env file

Create .env:

DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx

ğŸ”— 4. chain.py

Your LangChain chain definition.

# chain.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import os

def create_chain():
    """Build the DeepSeek chain"""

    model = ChatOpenAI(
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com",
        model="deepseek-chat",     # or deepseek-reasoner
        temperature=0.3,
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šAIåŠ©æ‰‹ï¼Œå›ç­”è¦å‡†ç¡®ç®€æ´ã€‚"),
        ("user", "{input}")
    ])

    chain = prompt | model
    return chain

ğŸ–¥ï¸ 5. server.py

Your LangServe FastAPI server.

# server.py
import os
from dotenv import load_dotenv
from fastapi import FastAPI
from langserve import add_routes

from chain import create_chain

# Load environment variables
load_dotenv()

app = FastAPI(
    title="DeepSeek LangServe API",
    description="Serve DeepSeek LLM using LangChain + LangServe",
    version="1.0.0",
)

# Add DeepSeek chain under /deepseek
add_routes(
    app,
    create_chain(),
    path="/deepseek"
)

@app.get("/")
def root():
    return {"message": "DeepSeek LangServe is running!"}

â–¶ï¸ 6. Start the server
uvicorn server:app --reload --port 8000

ğŸŒ 7. Test the API
ğŸŸ¦ A. Synchronous /invoke endpoint

POST:

http://localhost:8000/deepseek/invoke


Body:

{
  "input": "è§£é‡Šä¸€ä¸‹ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ"
}


Response:

{
  "output": "å¤©ç©ºå‘ˆè“è‰²æ˜¯å› ä¸ºç‘åˆ©æ•£å°„..."
}

ğŸŸ© B. Streaming endpoint

POST:

http://localhost:8000/deepseek/stream


Body:

{
  "input": "å†™ä¸€ä¸ª50å­—çš„æ•…äº‹"
}


You will receive streaming chunks.

ğŸŸ§ C. Web UI (Swagger)

Open:

http://localhost:8000/docs


You will see:

/deepseek/invoke

/deepseek/stream

/deepseek/batch

All generated automatically by LangServe.

âš¡ BONUS: Client Example (Python)
import requests

resp = requests.post(
    "http://localhost:8000/deepseek/invoke",
    json={"input": "è®²ä¸€ä¸ªç¬‘è¯"}
)

print(resp.json())

âœ” You now have a full local DeepSeek LLM API server!

This setup lets you:

Build your own OpenAI-like API powered by DeepSeek

Integrate with external tools

Deploy to cloud (Docker instructions available)

Use structured chains, RAG pipelines, tools, async, etc.
